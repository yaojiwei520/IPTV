name: Update IPTV Sources

on:
  schedule:
    # Runs every 15 minutes (UTC time, e.g., at XX:00, XX:15, XX:30, XX:45)
    - cron: '*/15 * * * *'
  workflow_dispatch: # Allows manual triggering from GitHub UI

jobs:
  update-and-clean:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          # Fetch all history to allow `add-and-commit` to work correctly
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Fetch and Clean IPTV data (Filtered Channels)
        id: fetch_clean_data
        run: |
          SOURCE_URL="https://raw.githubusercontent.com/hujingguang/ChinaIPTV/main/cnTV_AutoUpdate.m3u8"
          CLEANED_FILE="cnTV_AutoUpdate.m3u"
          RAW_FILE="raw_source.m3u8"

          echo "Step 1: Fetching raw data from $SOURCE_URL..."
          curl -s "$SOURCE_URL" > "$RAW_FILE"

          if [ ! -f "$RAW_FILE" ] || [ ! -s "$RAW_FILE" ]; then
              echo "Error: Failed to download source file or '$RAW_FILE' is empty."
              exit 1
          fi

          echo "Step 2: Cleaning and filtering the data..."
          # Start of Here-document for Python script
          python - <<EOF_PYTHON
          import re
          import os

          # Variables passed from Bash
          RAW_FILE = os.environ.get('RAW_SOURCE_FILE')
          CLEANED_FILE = os.environ.get('CLEANED_OUTPUT_FILE')

          # 定义要保留的频道名称和tvg-id关键字
          CHANNELS_TO_KEEP = [
              '湖南爱晚',
              '湖南娱乐频道',
              '金鹰卡通',
              '湖南国际频道',
              '长沙新闻频道'
          ]
          TVG_IDS_TO_KEEP = [ # 针对 'dianshiju.hunan' 这种可能是 tvg-id 的情况
              'dianshiju.hunan'
          ]

          output_lines = []
          current_extinf = None # 用于暂存 #EXTINF 行
          
          try:
              with open(RAW_FILE, 'r', encoding='utf-8') as f_in:
                  lines = f_in.readlines()
                  i = 0
                  while i < len(lines):
                      line = lines[i].strip()

                      if line.startswith('#EXTINF:'):
                          # 尝试提取频道名称 (","后面的部分)
                          channel_name_match = re.search(r',([^,]+)$', line)
                          channel_name = channel_name_match.group(1).strip() if channel_name_match else ''

                          # 尝试提取 tvg-id
                          tvg_id_match = re.search(r'tvg-id="([^"]*)"', line)
                          tvg_id = tvg_id_match.group(1).strip() if tvg_id_match else ''

                          # 判断是否需要保留这一组数据
                          should_keep = False
                          if channel_name in CHANNELS_TO_KEEP:
                              should_keep = True
                          if tvg_id in TVG_IDS_TO_KEEP:
                              should_keep = True
                          
                          if should_keep:
                              current_extinf = line # 暂存 EXTINF 行
                          else:
                              current_extinf = None # 不符合条件的，清空暂存，不保留
                          
                      elif (line.startswith('http://') or line.startswith('https://')) and current_extinf is not None:
                          # 如果当前行是URL，并且之前暂存的 #EXTINF 符合条件
                          output_lines.append(current_extinf)
                          output_lines.append(line)
                          current_extinf = None # 重置，准备处理下一组
                      else:
                          # 如果是其他行或者不符合要求的URL，则忽略
                          current_extinf = None # 如果上一行EXTINF不符合，当前URL也跳过
                          pass

                      i += 1
          except Exception as e:
              print(f'Error reading or processing raw file: {e}')
              exit(1)

          with open(CLEANED_FILE, 'w', encoding='utf-8') as f_out:
              f_out.write('#EXTM3U\n') # Add the M3U header
              for line in output_lines:
                  f_out.write(line + '\n')
          EOF_PYTHON # End of Here-document
          echo "Cleaned and filtered data saved to $CLEANED_FILE"
        env: # Pass shell variables as environment variables to the Python script
          RAW_SOURCE_FILE: "$RAW_FILE"
          CLEANED_OUTPUT_FILE: "$CLEANED_FILE"

      - name: Merge cleaned data with iptv.m3u and deduplicate
        run: |
          MERGED_FILE="iptv.m3u"
          CLEANED_FILE="cnTV_AutoUpdate.m3u"

          # Ensure iptv.m3u exists, initialize if not
          if [ ! -f "$MERGED_FILE" ]; then
              echo '#EXTM3U' > "$MERGED_FILE"
          fi

          echo "Merging $CLEANED_FILE into $MERGED_FILE..."
          # Append cleaned data to iptv.m3u, avoiding duplicate #EXTM3U header from cleaned file
          # Use awk to skip the first line (#EXTM3U) of cnTV_AutoUpdate.m3u if it exists
          awk 'NR > 1 || !/^#EXTM3U$/' "$CLEANED_FILE" >> "$MERGED_FILE"

          echo "Merging completed. Now deduplicating $MERGED_FILE..."
          # Deduplicate iptv.m3u in place
          python - <<EOF_PYTHON_DEDUP
          import re
          import os

          MERGED_FILE = os.environ.get('MERGED_OUTPUT_FILE')

          seen_entries = set()
          output_lines = ['#EXTM3U'] # Always start with one #EXTM3U header

          try:
              with open(MERGED_FILE, 'r', encoding='utf-8') as f_in:
                  lines = f_in.readlines()
                  i = 0
                  while i < len(lines):
                      line = lines[i].strip()
                      # Ignore empty lines
                      if not line:
                           i += 1
                           continue
                      
                      # Only process #EXTINF and its associated URL
                      if line.startswith('#EXTINF:'):
                          if i + 1 < len(lines):
                              next_line = lines[i+1].strip()
                              if next_line.startswith('http://') or next_line.startswith('https://'):
                                  # Create a unique key for the channel entry (EXTINF + URL)
                                  entry_key = (line, next_line)
                                  if entry_key not in seen_entries:
                                      seen_entries.add(entry_key)
                                      output_lines.append(line)
                                      output_lines.append(next_line)
                                  i += 1 # Skip the URL line since it's part of this entry
                          else:
                              # If #EXTINF is the last line or not followed by a URL, still add it
                              entry_key = (line, '') # Use empty string for URL if none
                              if entry_key not in seen_entries:
                                  seen_entries.add(entry_key)
                                  output_lines.append(line)
                      elif line == '#EXTM3U': # Skip subsequent #EXTM3U headers
                          pass
                      elif line.startswith('http://') or line.startswith('https://'): # Handle loose URLs without #EXTINF
                          entry_key = ('', line) # Treat as a standalone URL
                          if entry_key not in seen_entries:
                              seen_entries.add(entry_key)
                              output_lines.append(line)
                      
                      i += 1
          except Exception as e:
              print(f'Error deduplicating file: {e}')
              exit(1)

          with open(MERGED_FILE, 'w', encoding='utf-8') as f_out:
              f_out.write('\\n'.join(output_lines) + '\\n')
          EOF_PYTHON_DEDUP
          echo "Deduplication completed for $MERGED_FILE"
        env: # Pass shell variables as environment variables
          MERGED_OUTPUT_FILE: "$MERGED_FILE"


      - name: Update VersionLog
        run: |
          LOG_FILE="VersionLog.md"
          # Ensure VersionLog.md exists
          if [ ! -f "$LOG_FILE" ]; then
              echo "# Update Log" > "$LOG_FILE"
          fi
          # Append new log entry
          # GITHUB_REF_NAME provides branch name, or 'manual-run' for workflow_dispatch
          echo "## ${GITHUB_REF_NAME:-Manual Run} - Update on $(TZ='Asia/Shanghai' date '+%Y-%m-%d %H:%M:%S %Z')" >> "$LOG_FILE"
          echo "" >> "$LOG_FILE" # Add an empty line for better readability

      - name: Commit and push changes
        uses: EndBug/add-and-commit@v9
        with:
          # Specify files to add/commit. Ensure 'raw_source.m3u8' is NOT committed.
          add: 'cnTV_AutoUpdate.m3u iptv.m3u VersionLog.md'
          message: 'Auto-update IPTV sources (filtered ChinaIPTV) and log [skip ci]'
          default_author: github_actions
          # The GITHUB_TOKEN has permissions to push to the repository
          # It's automatically provided by GitHub Actions
          token: ${{ secrets.GITHUB_TOKEN }}
